{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##the OCR HANDLER FOR ARABIC TEXT\n",
    "from tessetact_pdf_processor import PDFTextExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_arabic_processor = PDFTextExtractor(dpi = 400, language='ara')\n",
    "\n",
    "pdf_arabic_processor.process_pdf('short_stories_ar.pdf', 'short_stories_ar.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "import time\n",
    "from llm import GeminiLLM\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "\n",
    "class ArabicTextCorrector:\n",
    "    \"\"\"\n",
    "    A class that provides functionality to correct Arabic text in chunks using an LLM model. \n",
    "    It handles reading, processing, splitting, and merging text files while allowing for corrections\n",
    "    with retry logic in case of errors during the correction process.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: GeminiLLM):\n",
    "        \"\"\"\n",
    "        Initializes the ArabicTextCorrector instance with a given model.\n",
    "\n",
    "        Args:\n",
    "            model (GeminiLLM): An instance of the GeminiLLM model used for text corrections.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "\n",
    "    def configure(self):\n",
    "        \"\"\"Configures the model for use. (Stub for potential future use)\"\"\"\n",
    "        pass\n",
    "\n",
    "    def read_text_from_file(self, file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Reads text from a file and returns it.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file to read from.\n",
    "\n",
    "        Returns:\n",
    "            str: The content of the file.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "\n",
    "    def save_text_to_file(self, text: str, file_path: str):\n",
    "        \"\"\"\n",
    "        Saves a string of text to a file, ensuring the directory exists.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to save.\n",
    "            file_path (str): The path to the file to save to.\n",
    "        \"\"\"\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(text)\n",
    "\n",
    "    def split_text(self, text: str, chunk_size: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Splits the text into chunks of a specified size.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be split.\n",
    "            chunk_size (int): The size of each chunk.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of text chunks.\n",
    "        \"\"\"\n",
    "        return textwrap.wrap(text, chunk_size)\n",
    "\n",
    "    def generate_correction_prompt(self, text: str, custom_prompt: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Generates a correction prompt for the model to process the text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be corrected.\n",
    "            custom_prompt (str): An optional custom prompt to be used for correction.\n",
    "\n",
    "        Returns:\n",
    "            str: A prompt that will be passed to the model for text correction.\n",
    "        \"\"\"\n",
    "        if custom_prompt:\n",
    "            return f\"{custom_prompt}\\n{text}\"\n",
    "        return f\"يرجى إعادة كتابة النص التالي بشكل صحيح دون أي ملاحظات أو توضيحات:\\n{text}\"\n",
    "\n",
    "    def correct_text(self, text: str, custom_prompt: str = None, max_retries: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        Corrects the provided text using the model, with retry logic in case of failure.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be corrected.\n",
    "            custom_prompt (str): An optional custom prompt for the correction.\n",
    "            max_retries (int): The maximum number of retry attempts in case of failure.\n",
    "\n",
    "        Returns:\n",
    "            str: The corrected text or an error message after the maximum retries.\n",
    "        \"\"\"\n",
    "        prompt = self.generate_correction_prompt(text, custom_prompt)\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model is not configured. Please call 'configure' first.\")\n",
    "        attempts = 0\n",
    "        while attempts < max_retries:\n",
    "            try:\n",
    "                response = self.model.generate_content(prompt)\n",
    "                return response\n",
    "            except Exception as e:\n",
    "                attempts += 1\n",
    "                print(f\"Error occurred while generating content (attempt {attempts}/{max_retries}): {e}\")\n",
    "                if attempts == max_retries:\n",
    "                    return f\"\\n\\n[تنبيه: لم يتم معالجة هذا الجزء بسبب خطأ بعد {max_retries} محاولات]\\n\\n{text}\"\n",
    "\n",
    "    def process_file(self, input_file: str, output_file: str, output_folder: str, chunk_size: int = 1000, custom_prompt: str = None):\n",
    "        \"\"\"\n",
    "        Processes the input file, splits it into chunks, corrects each chunk, and saves the output to a folder.\n",
    "\n",
    "        Args:\n",
    "            input_file (str): The path to the input text file.\n",
    "            output_file (str): The path to the final output file.\n",
    "            output_folder (str): The folder to save the corrected chunks.\n",
    "            chunk_size (int): The size of each chunk in characters.\n",
    "            custom_prompt (str): An optional custom prompt for text correction.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        text = self.read_text_from_file(input_file)\n",
    "        chunks = self.split_text(text, chunk_size)\n",
    "\n",
    "        corrected_text = \"\"\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            corrected_chunk = self.correct_text(chunk, custom_prompt)\n",
    "            corrected_text += corrected_chunk\n",
    "            output_path = os.path.join(output_folder, f'corrected_chunk_{i}.txt')\n",
    "            self.save_text_to_file(corrected_chunk, output_path)\n",
    "            print(f\"Processed chunk {i + 1} of {len(chunks)}\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        runtime = end_time - start_time\n",
    "        log_message = f\"{os.path.basename(__file__)} --> time taken: {runtime:.2f} seconds --> start of the run: {time.ctime(start_time)}\\n\"\n",
    "        self.log_runtime(log_message)\n",
    "\n",
    "    def save_chunks_without_processing(self, input_file: str, chunk_numbers: List[int], output_folder: str, chunk_size: int = 1000):\n",
    "        \"\"\"\n",
    "        Saves specified chunks of the input file without processing.\n",
    "\n",
    "        Args:\n",
    "            input_file (str): The path to the input text file.\n",
    "            chunk_numbers (List[int]): A list of chunk numbers to save.\n",
    "            output_folder (str): The folder to save the chunks.\n",
    "            chunk_size (int): The size of each chunk in characters.\n",
    "        \"\"\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        text = self.read_text_from_file(input_file)\n",
    "        chunks = self.split_text(text, chunk_size)\n",
    "\n",
    "        for index in chunk_numbers:\n",
    "            if index < 1 or index > len(chunks):\n",
    "                print(f\"Chunk number {index} is out of range.\")\n",
    "                continue\n",
    "            chunk = chunks[index - 1]\n",
    "            output_path = os.path.join(output_folder, f'corrected_chunk_{index}.txt')\n",
    "            self.save_text_to_file(chunk, output_path)\n",
    "            print(f\"Saved chunk {index} of {len(chunks)}\")\n",
    "\n",
    "    def log_runtime(self, log_message: str):\n",
    "        \"\"\"\n",
    "        Logs the runtime information into a file.\n",
    "\n",
    "        Args:\n",
    "            log_message (str): The message to be logged.\n",
    "        \"\"\"\n",
    "        with open(\"runtime.log\", 'a', encoding='utf-8') as log_file:\n",
    "            log_file.write(log_message)\n",
    "\n",
    "    def merge_chunks(self, input_folder: str, output_file: str):\n",
    "        \"\"\"\n",
    "        Merges all chunks in the input folder into a single file, sorted by chunk number.\n",
    "\n",
    "        Args:\n",
    "            input_folder (str): The folder containing chunk files.\n",
    "            output_file (str): The file to save the merged output.\n",
    "        \"\"\"\n",
    "        files = os.listdir(input_folder)\n",
    "        sorted_files = sorted(files, key=lambda x: int(re.search(r'corrected_chunk_(\\d+)\\.txt', x).group(1)))\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as output:\n",
    "            for file in sorted_files:\n",
    "                with open(os.path.join(input_folder, file), 'r', encoding='utf-8') as input_file:\n",
    "                    print(f\"Merging {file}\")\n",
    "                    output.write(input_file.read())\n",
    "                    output.write(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # Load environment variables\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "# Instantiate and configure GeminiLLM\n",
    "model = GeminiLLM(model_name='gemini-1.0-pro-latest')\n",
    "model.configure(api_key=gemini_api_key)\n",
    "corrector = ArabicTextCorrector(model)\n",
    "corrector.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the file with custom prompt and logging runtime\n",
    "corrector.process_file(\n",
    "    input_file=\"short_stories_ar.txt\", \n",
    "    output_file=\"corrected_short_ar_stories.txt\", \n",
    "    output_folder=\"correct_ar_stories\",\n",
    "    chunk_size=1000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving specific chunks without processing\n",
    "\n",
    "chunk_numbers = [10,94,95]\n",
    "\n",
    "corrector.save_chunks_without_processing(input_file=\"taw_hist.txt\", chunk_numbers=chunk_numbers, output_folder=\"correct_hist\", chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: MERGE THE FILE AFTER MANUALLY EDITING UN-USABLE CHUNKS for both bio and hist\n",
    "\n",
    "##manual fix is done so now we can merge the files\n",
    "\n",
    "corrector.merge_chunks(input_folder=\"correct_bio\", output_file=\"corrected_bio.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##chroma class and its util classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chroma_text_processing import RecursiveCharacterTextSplitterAdapter, ChromaInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the interface\n",
    "text_splitter = RecursiveCharacterTextSplitterAdapter(chunk_size=200, chunk_overlap=20)\n",
    "\n",
    "chroma_interface = ChromaInterface(\"taw_bio\",\n",
    "                                   \"DB/chroma_db\",\n",
    "                                       text_splitter=text_splitter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_interface.client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the embeddings for the bio chunks\n",
    "\n",
    "chroma_interface.add_documents_from_files(\n",
    "    file_paths=[\"corrected_bio.txt\"],\n",
    "    metadatas=[{\"source\": \"corrected_bio.txt\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the user for input instead of hardcoding the query_text\n",
    "query_text = input(\"Please enter your query: \")\n",
    "\n",
    "# Execute the query using the user's input\n",
    "results = chroma_interface.query(query_text, n_results=5)\n",
    "\n",
    "# Print the results\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Result {i + 1}:\\n{result}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
