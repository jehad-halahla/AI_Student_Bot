{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "import time\n",
    "from llm import GeminiLLM\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "\n",
    "class ArabicTextCorrector:\n",
    "    \"\"\"\n",
    "    A class that provides functionality to correct Arabic text in chunks using an LLM model. \n",
    "    It handles reading, processing, splitting, and merging text files while allowing for corrections\n",
    "    with retry logic in case of errors during the correction process.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: GeminiLLM):\n",
    "        \"\"\"\n",
    "        Initializes the ArabicTextCorrector instance with a given model.\n",
    "\n",
    "        Args:\n",
    "            model (GeminiLLM): An instance of the GeminiLLM model used for text corrections.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "\n",
    "    def configure(self):\n",
    "        \"\"\"Configures the model for use. (Stub for potential future use)\"\"\"\n",
    "        pass\n",
    "\n",
    "    def read_text_from_file(self, file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Reads text from a file and returns it.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file to read from.\n",
    "\n",
    "        Returns:\n",
    "            str: The content of the file.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "\n",
    "    def save_text_to_file(self, text: str, file_path: str):\n",
    "        \"\"\"\n",
    "        Saves a string of text to a file, ensuring the directory exists.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to save.\n",
    "            file_path (str): The path to the file to save to.\n",
    "        \"\"\"\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(text)\n",
    "\n",
    "    def split_text(self, text: str, chunk_size: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Splits the text into chunks of a specified size.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be split.\n",
    "            chunk_size (int): The size of each chunk.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of text chunks.\n",
    "        \"\"\"\n",
    "        return textwrap.wrap(text, chunk_size)\n",
    "\n",
    "    def generate_correction_prompt(self, text: str, custom_prompt: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Generates a correction prompt for the model to process the text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be corrected.\n",
    "            custom_prompt (str): An optional custom prompt to be used for correction.\n",
    "\n",
    "        Returns:\n",
    "            str: A prompt that will be passed to the model for text correction.\n",
    "        \"\"\"\n",
    "        if custom_prompt:\n",
    "            return f\"{custom_prompt}\\n{text}\"\n",
    "        return f\"يرجى إعادة كتابة النص التالي بشكل صحيح دون أي ملاحظات أو توضيحات:\\n{text}\"\n",
    "\n",
    "    def correct_text(self, text: str, custom_prompt: str = None, max_retries: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        Corrects the provided text using the model, with retry logic in case of failure.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be corrected.\n",
    "            custom_prompt (str): An optional custom prompt for the correction.\n",
    "            max_retries (int): The maximum number of retry attempts in case of failure.\n",
    "\n",
    "        Returns:\n",
    "            str: The corrected text or an error message after the maximum retries.\n",
    "        \"\"\"\n",
    "        prompt = self.generate_correction_prompt(text, custom_prompt)\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model is not configured. Please call 'configure' first.\")\n",
    "        attempts = 0\n",
    "        while attempts < max_retries:\n",
    "            try:\n",
    "                response = self.model.generate_content(prompt)\n",
    "                return response\n",
    "            except Exception as e:\n",
    "                attempts += 1\n",
    "                print(f\"Error occurred while generating content (attempt {attempts}/{max_retries}): {e}\")\n",
    "                if attempts == max_retries:\n",
    "                    return f\"\\n\\n[تنبيه: لم يتم معالجة هذا الجزء بسبب خطأ بعد {max_retries} محاولات]\\n\\n{text}\"\n",
    "\n",
    "    def process_file(self, input_file: str, output_file: str, output_folder: str, chunk_size: int = 1000, custom_prompt: str = None):\n",
    "        \"\"\"\n",
    "        Processes the input file, splits it into chunks, corrects each chunk, and saves the output to a folder.\n",
    "\n",
    "        Args:\n",
    "            input_file (str): The path to the input text file.\n",
    "            output_file (str): The path to the final output file.\n",
    "            output_folder (str): The folder to save the corrected chunks.\n",
    "            chunk_size (int): The size of each chunk in characters.\n",
    "            custom_prompt (str): An optional custom prompt for text correction.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        text = self.read_text_from_file(input_file)\n",
    "        chunks = self.split_text(text, chunk_size)\n",
    "\n",
    "        corrected_text = \"\"\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            corrected_chunk = self.correct_text(chunk, custom_prompt)\n",
    "            corrected_text += corrected_chunk\n",
    "            output_path = os.path.join(output_folder, f'corrected_chunk_{i}.txt')\n",
    "            self.save_text_to_file(corrected_chunk, output_path)\n",
    "            print(f\"Processed chunk {i + 1} of {len(chunks)}\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        runtime = end_time - start_time\n",
    "        log_message = f\"{os.path.basename(__file__)} --> time taken: {runtime:.2f} seconds --> start of the run: {time.ctime(start_time)}\\n\"\n",
    "        self.log_runtime(log_message)\n",
    "\n",
    "    def save_chunks_without_processing(self, input_file: str, chunk_numbers: List[int], output_folder: str, chunk_size: int = 1000):\n",
    "        \"\"\"\n",
    "        Saves specified chunks of the input file without processing.\n",
    "\n",
    "        Args:\n",
    "            input_file (str): The path to the input text file.\n",
    "            chunk_numbers (List[int]): A list of chunk numbers to save.\n",
    "            output_folder (str): The folder to save the chunks.\n",
    "            chunk_size (int): The size of each chunk in characters.\n",
    "        \"\"\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        text = self.read_text_from_file(input_file)\n",
    "        chunks = self.split_text(text, chunk_size)\n",
    "\n",
    "        for index in chunk_numbers:\n",
    "            if index < 1 or index > len(chunks):\n",
    "                print(f\"Chunk number {index} is out of range.\")\n",
    "                continue\n",
    "            chunk = chunks[index - 1]\n",
    "            output_path = os.path.join(output_folder, f'corrected_chunk_{index}.txt')\n",
    "            self.save_text_to_file(chunk, output_path)\n",
    "            print(f\"Saved chunk {index} of {len(chunks)}\")\n",
    "\n",
    "    def log_runtime(self, log_message: str):\n",
    "        \"\"\"\n",
    "        Logs the runtime information into a file.\n",
    "\n",
    "        Args:\n",
    "            log_message (str): The message to be logged.\n",
    "        \"\"\"\n",
    "        with open(\"runtime.log\", 'a', encoding='utf-8') as log_file:\n",
    "            log_file.write(log_message)\n",
    "\n",
    "    def merge_chunks(self, input_folder: str, output_file: str):\n",
    "        \"\"\"\n",
    "        Merges all chunks in the input folder into a single file, sorted by chunk number.\n",
    "\n",
    "        Args:\n",
    "            input_folder (str): The folder containing chunk files.\n",
    "            output_file (str): The file to save the merged output.\n",
    "        \"\"\"\n",
    "        files = os.listdir(input_folder)\n",
    "        sorted_files = sorted(files, key=lambda x: int(re.search(r'corrected_chunk_(\\d+)\\.txt', x).group(1)))\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as output:\n",
    "            for file in sorted_files:\n",
    "                with open(os.path.join(input_folder, file), 'r', encoding='utf-8') as input_file:\n",
    "                    print(f\"Merging {file}\")\n",
    "                    output.write(input_file.read())\n",
    "                    output.write(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic Text Corrector\n",
    "\n",
    "This project contains a Python class, `ArabicTextCorrector`, that leverages a Language Model (LLM) to correct Arabic text in chunks. It processes input text files, splits them into chunks, corrects the text, and saves the results into multiple files. Additionally, it can merge the chunks into a single file in the correct order.\n",
    "\n",
    "## Features\n",
    "\n",
    "- Read text from files.\n",
    "- Split text into chunks of a specified size.\n",
    "- Correct Arabic text using a Language Model with retry logic in case of failures.\n",
    "- Save corrected text chunks to separate files.\n",
    "- Merge chunk files into a single file in the correct order.\n",
    "- Log runtime information for the text correction process.\n",
    "- Save specific chunks without processing.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Python 3.7 or higher\n",
    "- The following Python packages:\n",
    "  - `llm` (for the GeminiLLM model)\n",
    "  - `python-dotenv` (for managing environment variables)\n",
    "\n",
    "### Installing Requirements\n",
    "\n",
    "#### Install all dependencies at once:\n",
    "\n",
    "To install all required packages at once, use the following command:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # Load environment variables\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "# Instantiate and configure GeminiLLM\n",
    "model = GeminiLLM(model_name='gemini-1.0-pro-latest')\n",
    "model.configure(api_key=gemini_api_key)\n",
    "corrector = ArabicTextCorrector(model)\n",
    "corrector.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the file with custom prompt and logging runtime\n",
    "corrector.process_file(\n",
    "    input_file=\"taw_hist.txt\", \n",
    "    output_file=\"corrected_hist.txt\", \n",
    "    output_folder=\"correct_hist_retry\",\n",
    "    chunk_size=1000\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving specific chunks without processing\n",
    "\n",
    "chunk_numbers = [10,94,95]\n",
    "\n",
    "corrector.save_chunks_without_processing(input_file=\"taw_hist.txt\", chunk_numbers=chunk_numbers, output_folder=\"correct_hist\", chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging corrected_chunk_0.txt\n",
      "Merging corrected_chunk_1.txt\n",
      "Merging corrected_chunk_2.txt\n",
      "Merging corrected_chunk_3.txt\n",
      "Merging corrected_chunk_4.txt\n",
      "Merging corrected_chunk_5.txt\n",
      "Merging corrected_chunk_6.txt\n",
      "Merging corrected_chunk_7.txt\n",
      "Merging corrected_chunk_8.txt\n",
      "Merging corrected_chunk_9.txt\n",
      "Merging corrected_chunk_10.txt\n",
      "Merging corrected_chunk_11.txt\n",
      "Merging corrected_chunk_12.txt\n",
      "Merging corrected_chunk_13.txt\n",
      "Merging corrected_chunk_14.txt\n",
      "Merging corrected_chunk_15.txt\n",
      "Merging corrected_chunk_16.txt\n",
      "Merging corrected_chunk_17.txt\n",
      "Merging corrected_chunk_18.txt\n",
      "Merging corrected_chunk_19.txt\n",
      "Merging corrected_chunk_20.txt\n",
      "Merging corrected_chunk_21.txt\n",
      "Merging corrected_chunk_22.txt\n",
      "Merging corrected_chunk_23.txt\n",
      "Merging corrected_chunk_24.txt\n",
      "Merging corrected_chunk_25.txt\n",
      "Merging corrected_chunk_26.txt\n",
      "Merging corrected_chunk_27.txt\n",
      "Merging corrected_chunk_28.txt\n",
      "Merging corrected_chunk_29.txt\n",
      "Merging corrected_chunk_30.txt\n",
      "Merging corrected_chunk_31.txt\n",
      "Merging corrected_chunk_32.txt\n",
      "Merging corrected_chunk_33.txt\n",
      "Merging corrected_chunk_34.txt\n",
      "Merging corrected_chunk_35.txt\n",
      "Merging corrected_chunk_36.txt\n",
      "Merging corrected_chunk_37.txt\n",
      "Merging corrected_chunk_38.txt\n",
      "Merging corrected_chunk_39.txt\n",
      "Merging corrected_chunk_40.txt\n",
      "Merging corrected_chunk_41.txt\n",
      "Merging corrected_chunk_42.txt\n",
      "Merging corrected_chunk_43.txt\n",
      "Merging corrected_chunk_44.txt\n",
      "Merging corrected_chunk_45.txt\n",
      "Merging corrected_chunk_46.txt\n",
      "Merging corrected_chunk_47.txt\n",
      "Merging corrected_chunk_48.txt\n",
      "Merging corrected_chunk_49.txt\n",
      "Merging corrected_chunk_50.txt\n",
      "Merging corrected_chunk_51.txt\n",
      "Merging corrected_chunk_52.txt\n",
      "Merging corrected_chunk_53.txt\n",
      "Merging corrected_chunk_54.txt\n",
      "Merging corrected_chunk_55.txt\n",
      "Merging corrected_chunk_56.txt\n",
      "Merging corrected_chunk_57.txt\n",
      "Merging corrected_chunk_58.txt\n",
      "Merging corrected_chunk_59.txt\n",
      "Merging corrected_chunk_60.txt\n",
      "Merging corrected_chunk_61.txt\n",
      "Merging corrected_chunk_62.txt\n",
      "Merging corrected_chunk_63.txt\n",
      "Merging corrected_chunk_64.txt\n",
      "Merging corrected_chunk_65.txt\n",
      "Merging corrected_chunk_66.txt\n",
      "Merging corrected_chunk_67.txt\n",
      "Merging corrected_chunk_68.txt\n",
      "Merging corrected_chunk_69.txt\n",
      "Merging corrected_chunk_70.txt\n",
      "Merging corrected_chunk_71.txt\n",
      "Merging corrected_chunk_72.txt\n",
      "Merging corrected_chunk_73.txt\n",
      "Merging corrected_chunk_74.txt\n",
      "Merging corrected_chunk_75.txt\n",
      "Merging corrected_chunk_76.txt\n",
      "Merging corrected_chunk_77.txt\n",
      "Merging corrected_chunk_78.txt\n",
      "Merging corrected_chunk_79.txt\n",
      "Merging corrected_chunk_80.txt\n",
      "Merging corrected_chunk_81.txt\n",
      "Merging corrected_chunk_82.txt\n",
      "Merging corrected_chunk_83.txt\n",
      "Merging corrected_chunk_84.txt\n",
      "Merging corrected_chunk_85.txt\n",
      "Merging corrected_chunk_86.txt\n",
      "Merging corrected_chunk_87.txt\n",
      "Merging corrected_chunk_88.txt\n",
      "Merging corrected_chunk_89.txt\n",
      "Merging corrected_chunk_90.txt\n",
      "Merging corrected_chunk_91.txt\n",
      "Merging corrected_chunk_92.txt\n",
      "Merging corrected_chunk_93.txt\n",
      "Merging corrected_chunk_94.txt\n",
      "Merging corrected_chunk_95.txt\n",
      "Merging corrected_chunk_96.txt\n",
      "Merging corrected_chunk_97.txt\n",
      "Merging corrected_chunk_98.txt\n",
      "Merging corrected_chunk_99.txt\n",
      "Merging corrected_chunk_100.txt\n",
      "Merging corrected_chunk_101.txt\n",
      "Merging corrected_chunk_102.txt\n",
      "Merging corrected_chunk_103.txt\n",
      "Merging corrected_chunk_104.txt\n",
      "Merging corrected_chunk_105.txt\n",
      "Merging corrected_chunk_106.txt\n",
      "Merging corrected_chunk_107.txt\n",
      "Merging corrected_chunk_108.txt\n",
      "Merging corrected_chunk_109.txt\n",
      "Merging corrected_chunk_110.txt\n",
      "Merging corrected_chunk_111.txt\n",
      "Merging corrected_chunk_112.txt\n",
      "Merging corrected_chunk_113.txt\n",
      "Merging corrected_chunk_114.txt\n",
      "Merging corrected_chunk_115.txt\n",
      "Merging corrected_chunk_116.txt\n",
      "Merging corrected_chunk_117.txt\n",
      "Merging corrected_chunk_118.txt\n",
      "Merging corrected_chunk_119.txt\n",
      "Merging corrected_chunk_120.txt\n",
      "Merging corrected_chunk_121.txt\n",
      "Merging corrected_chunk_122.txt\n",
      "Merging corrected_chunk_123.txt\n",
      "Merging corrected_chunk_124.txt\n",
      "Merging corrected_chunk_125.txt\n",
      "Merging corrected_chunk_126.txt\n",
      "Merging corrected_chunk_127.txt\n",
      "Merging corrected_chunk_128.txt\n",
      "Merging corrected_chunk_129.txt\n",
      "Merging corrected_chunk_130.txt\n",
      "Merging corrected_chunk_131.txt\n",
      "Merging corrected_chunk_132.txt\n",
      "Merging corrected_chunk_133.txt\n",
      "Merging corrected_chunk_134.txt\n",
      "Merging corrected_chunk_135.txt\n",
      "Merging corrected_chunk_136.txt\n",
      "Merging corrected_chunk_137.txt\n",
      "Merging corrected_chunk_138.txt\n",
      "Merging corrected_chunk_139.txt\n",
      "Merging corrected_chunk_140.txt\n",
      "Merging corrected_chunk_141.txt\n",
      "Merging corrected_chunk_142.txt\n",
      "Merging corrected_chunk_143.txt\n",
      "Merging corrected_chunk_144.txt\n",
      "Merging corrected_chunk_145.txt\n",
      "Merging corrected_chunk_146.txt\n",
      "Merging corrected_chunk_147.txt\n",
      "Merging corrected_chunk_148.txt\n",
      "Merging corrected_chunk_149.txt\n",
      "Merging corrected_chunk_150.txt\n",
      "Merging corrected_chunk_151.txt\n",
      "Merging corrected_chunk_152.txt\n",
      "Merging corrected_chunk_153.txt\n",
      "Merging corrected_chunk_154.txt\n",
      "Merging corrected_chunk_155.txt\n",
      "Merging corrected_chunk_156.txt\n",
      "Merging corrected_chunk_157.txt\n",
      "Merging corrected_chunk_158.txt\n",
      "Merging corrected_chunk_159.txt\n",
      "Merging corrected_chunk_160.txt\n",
      "Merging corrected_chunk_161.txt\n",
      "Merging corrected_chunk_162.txt\n",
      "Merging corrected_chunk_163.txt\n",
      "Merging corrected_chunk_164.txt\n",
      "Merging corrected_chunk_165.txt\n",
      "Merging corrected_chunk_166.txt\n",
      "Merging corrected_chunk_167.txt\n",
      "Merging corrected_chunk_168.txt\n",
      "Merging corrected_chunk_169.txt\n",
      "Merging corrected_chunk_170.txt\n",
      "Merging corrected_chunk_171.txt\n",
      "Merging corrected_chunk_172.txt\n",
      "Merging corrected_chunk_173.txt\n",
      "Merging corrected_chunk_174.txt\n",
      "Merging corrected_chunk_175.txt\n",
      "Merging corrected_chunk_176.txt\n",
      "Merging corrected_chunk_177.txt\n",
      "Merging corrected_chunk_178.txt\n",
      "Merging corrected_chunk_179.txt\n",
      "Merging corrected_chunk_180.txt\n",
      "Merging corrected_chunk_181.txt\n",
      "Merging corrected_chunk_182.txt\n",
      "Merging corrected_chunk_183.txt\n",
      "Merging corrected_chunk_184.txt\n",
      "Merging corrected_chunk_185.txt\n",
      "Merging corrected_chunk_186.txt\n",
      "Merging corrected_chunk_187.txt\n"
     ]
    }
   ],
   "source": [
    "#TODO: MERGE THE FILE AFTER MANUALLY EDITING UN-USABLE CHUNKS for both bio and hist\n",
    "\n",
    "##manual fix is done so now we can merge the files\n",
    "\n",
    "corrector.merge_chunks(input_folder=\"correct_bio\", output_file=\"corrected_bio.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Optional\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, NLTKTextSplitter\n",
    "\n",
    "# Define a base TextSplitter class\n",
    "class TextSplitter:\n",
    "    \"\"\"\n",
    "    Base class for splitting text into chunks.\n",
    "    Subclasses should implement the split_text method.\n",
    "    \"\"\"\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        raise NotImplementedError(\"Subclasses should implement this method.\")\n",
    "\n",
    "# Implement specific text splitters\n",
    "class RecursiveCharacterTextSplitterAdapter(TextSplitter):\n",
    "    \"\"\"\n",
    "    Adapter for splitting text using the RecursiveCharacterTextSplitter from Langchain.\n",
    "    \"\"\"\n",
    "    def __init__(self, chunk_size: int = 100, chunk_overlap: int = 20):\n",
    "        \"\"\"\n",
    "        Initializes the RecursiveCharacterTextSplitterAdapter with the specified chunk size and overlap.\n",
    "        \n",
    "        Args:\n",
    "            chunk_size (int): Maximum size of each chunk.\n",
    "            chunk_overlap (int): Number of characters to overlap between chunks.\n",
    "        \"\"\"\n",
    "        self.splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            is_separator_regex=True\n",
    "        )\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Splits the given text into chunks using the RecursiveCharacterTextSplitter.\n",
    "\n",
    "        Args:\n",
    "            text (str): Text to split.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: List of text chunks.\n",
    "        \"\"\"\n",
    "        return self.splitter.split_text(text)\n",
    "\n",
    "class NLTKTextSplitterAdapter(TextSplitter):\n",
    "    \"\"\"\n",
    "    Adapter for splitting text using the NLTKTextSplitter from Langchain.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the NLTKTextSplitterAdapter.\n",
    "        \"\"\"\n",
    "        self.splitter = NLTKTextSplitter()\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Splits the given text into chunks using the NLTKTextSplitter.\n",
    "\n",
    "        Args:\n",
    "            text (str): Text to split.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: List of text chunks.\n",
    "        \"\"\"\n",
    "        return self.splitter.split_text(text)\n",
    "\n",
    "# Define a custom embedding function\n",
    "class CustomSentenceTransformerEmbedding(embedding_functions.EmbeddingFunction):\n",
    "    \"\"\"\n",
    "    Custom embedding function that generates embeddings using the SentenceTransformer model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"):\n",
    "        \"\"\"\n",
    "        Initializes the CustomSentenceTransformerEmbedding with the specified model.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the SentenceTransformer model to use.\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def __call__(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Generates embeddings for a list of texts.\n",
    "\n",
    "        Args:\n",
    "            texts (List[str]): List of input texts.\n",
    "\n",
    "        Returns:\n",
    "            List[List[float]]: List of embeddings for each input text.\n",
    "        \"\"\"\n",
    "        embeddings = self.model.encode(texts)\n",
    "        return embeddings.tolist()\n",
    "\n",
    "# Define the ChromaInterface class\n",
    "class ChromaInterface:\n",
    "    \"\"\"\n",
    "    Interface for interacting with ChromaDB to store and query document embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, collection_name: str, persist_directory: str, text_splitter: TextSplitter):\n",
    "        \"\"\"\n",
    "        Initializes the ChromaInterface with a persistent ChromaDB collection and a text splitter.\n",
    "\n",
    "        Args:\n",
    "            collection_name (str): Name of the collection in ChromaDB.\n",
    "            persist_directory (str): Directory where the ChromaDB data will be stored.\n",
    "            text_splitter (TextSplitter): Text splitter used to divide documents into chunks.\n",
    "        \"\"\"\n",
    "        self.client = chromadb.PersistentClient(path=persist_directory)\n",
    "        self.embedding_function = CustomSentenceTransformerEmbedding()\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            embedding_function=self.embedding_function\n",
    "        )\n",
    "        self.text_splitter = text_splitter\n",
    "\n",
    "    def add_documents_from_files(self, file_paths: List[str], metadatas: Optional[List[Dict[str, str]]] = None):\n",
    "        \"\"\"\n",
    "        Adds documents from the specified files into the ChromaDB collection after splitting them into chunks.\n",
    "        \n",
    "        Args:\n",
    "            file_paths (List[str]): List of file paths containing the documents to add.\n",
    "            metadatas (Optional[List[Dict[str, str]]]): List of metadata dictionaries corresponding to each file.\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "        ids = []\n",
    "        id_counter = 0  # Initialize a counter for unique IDs\n",
    "\n",
    "        # Process each file\n",
    "        for file_path in file_paths:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                # Split the content into chunks\n",
    "                split_texts = self.text_splitter.split_text(content)\n",
    "                documents.extend(split_texts)\n",
    "                \n",
    "                # Generate unique IDs for each chunk\n",
    "                for _ in split_texts:\n",
    "                    ids.append(f\"{os.path.basename(file_path)}_{id_counter}\")\n",
    "                    id_counter += 1  # Increment the counter for each chunk\n",
    "\n",
    "        if metadatas is None:\n",
    "            # If no metadata is provided, use the file path as the source metadata\n",
    "            metadatas = [{\"source\": file_path} for file_path in file_paths]\n",
    "\n",
    "        # Adjust metadatas to match the number of document chunks\n",
    "        extended_metadatas = []\n",
    "        for i, file_path in enumerate(file_paths):\n",
    "            metadata = metadatas[i] if metadatas and i < len(metadatas) else {\"source\": file_path}\n",
    "            # Duplicate the metadata for all chunks of the same document\n",
    "            extended_metadatas.extend([metadata] * len(self.text_splitter.split_text(open(file_path, 'r', encoding='utf-8').read())))\n",
    "\n",
    "        # Add documents and their metadata into the collection\n",
    "        self.collection.add(\n",
    "            documents=documents,\n",
    "            metadatas=extended_metadatas,\n",
    "            ids=ids\n",
    "        )\n",
    "\n",
    "    def query(self, query_text: str, n_results: int = 30) -> List[str]:\n",
    "        \"\"\"\n",
    "        Queries the ChromaDB collection for the most relevant documents based on the query text.\n",
    "        \n",
    "        Args:\n",
    "            query_text (str): The text query for searching relevant documents.\n",
    "            n_results (int): The number of results to return (default is 30).\n",
    "\n",
    "        Returns:\n",
    "            List[str]: List of relevant document chunks.\n",
    "        \"\"\"\n",
    "        results = self.collection.query(\n",
    "            query_texts=[query_text],\n",
    "            n_results=n_results\n",
    "        )['documents']\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the interface\n",
    "text_splitter = RecursiveCharacterTextSplitterAdapter(chunk_size=200, chunk_overlap=20)\n",
    "\n",
    "chroma_interface = ChromaInterface(\"taw_bio\",\n",
    "                                   \"DB/chroma_db\",\n",
    "                                       text_splitter=text_splitter\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(id=2d40b393-43fa-4837-a034-711c890415aa, name=taw_hist),\n",
       " Collection(id=ebf733d0-abe7-4952-8c42-9ee3538119d8, name=taw_bio)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_interface.client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to add documents: 47.45 seconds\n"
     ]
    }
   ],
   "source": [
    "#generate the embeddings for the bio chunks\n",
    "\n",
    "start = time.time()\n",
    "chroma_interface.add_documents_from_files(\n",
    "    file_paths=[\"corrected_bio.txt\"],\n",
    "    metadatas=[{\"source\": \"corrected_bio.txt\"}]\n",
    ")\n",
    "end = time.time()\n",
    "print(f\"Time taken to add documents: {end - start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "['ويُعرف هذا النوع بالتخمر.', '**الأمراض الحساسية:**', '**الدورة الاندماجية:**\\n\\nتُعرف الدورة الاندماجية أيضًا باسم التكاثر المعتدل أو الليسوجيني، وتتميز بما يلي:', '- **مفاصل غضروفية:** متحد نوعًا ما بنسيج غضروفي، على سبيل المثال، الارتفاق العاني.', '* **الأعضاء الليمفاوية:**\\n    * نخاع العظم: ينتج الخلايا الليمفاوية الجذعية.\\n    * العقد الليمفاوية: تصفي الليمف من مسببات الأمراض وتحتوي على خلايا بلازمية ومساعِدة وقتالة.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#query the bio chunks\n",
    "query_text = \"ما هي الميتوكوندريا؟\"\n",
    "results = chroma_interface.query(query_text, n_results=5)\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Result {i + 1}:\\n{result}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
