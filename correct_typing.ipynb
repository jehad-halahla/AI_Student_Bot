{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##the OCR HANDLER FOR ARABIC TEXT\n",
    "from tessetact_pdf_processor import PDFTextExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page 1\n",
      "Processing page 2\n",
      "Processing page 3\n",
      "Processing page 4\n",
      "Processing page 5\n",
      "Processing page 6\n",
      "Processing page 7\n",
      "Processing page 8\n",
      "Processing page 9\n",
      "Processing page 10\n",
      "Processing page 11\n",
      "Processing page 12\n",
      "Processing page 13\n",
      "Processing page 14\n",
      "Processing page 15\n",
      "Processing page 16\n",
      "Processing page 17\n",
      "Processing page 18\n",
      "Processing page 19\n",
      "Processing page 20\n",
      "Processing page 21\n",
      "Processing page 22\n",
      "Processing page 23\n",
      "Processing page 24\n",
      "Processing page 25\n",
      "Processing page 26\n",
      "Processing page 27\n",
      "Processing page 28\n",
      "Processing page 29\n",
      "Processing page 30\n",
      "Processing page 31\n",
      "Processing page 32\n",
      "Processing page 33\n",
      "Processing page 34\n",
      "Processing page 35\n",
      "Processing page 36\n",
      "Processing page 37\n",
      "Processing page 38\n",
      "Processing page 39\n",
      "Processing page 40\n",
      "Processing page 41\n",
      "Processing page 42\n",
      "Processing page 43\n",
      "Processing page 44\n",
      "Processing page 45\n",
      "Processing page 46\n",
      "Processing page 47\n",
      "Processing page 48\n",
      "Processing page 49\n",
      "Processing page 50\n",
      "Processing page 51\n",
      "Processing page 52\n",
      "Processing page 53\n",
      "Processing page 54\n",
      "Processing page 55\n",
      "Processing page 56\n",
      "Processing page 57\n",
      "Processing page 58\n",
      "Processing page 59\n",
      "Processing page 60\n",
      "Processing page 61\n",
      "Processing page 62\n",
      "Processing page 63\n",
      "Processing page 64\n",
      "Processing page 65\n",
      "Processing page 66\n",
      "Processing page 67\n",
      "Processing page 68\n",
      "Processing page 69\n",
      "Processing page 70\n",
      "Processing page 71\n",
      "Processing page 72\n",
      "Processing page 73\n",
      "Processing page 74\n",
      "Processing page 75\n",
      "Processing page 76\n",
      "Processing page 77\n",
      "Processing page 78\n",
      "Processing page 79\n",
      "Processing page 80\n",
      "Processing page 81\n",
      "Processing page 82\n",
      "Processing page 83\n",
      "Processing page 84\n",
      "Processing page 85\n",
      "Processing page 86\n",
      "Processing page 87\n",
      "Processing page 88\n",
      "Processing page 89\n",
      "Processing page 90\n",
      "Processing page 91\n",
      "Processing page 92\n",
      "Processing page 93\n",
      "Processing page 94\n",
      "Processing page 95\n",
      "Processing page 96\n",
      "Processing page 97\n",
      "Processing page 98\n",
      "Processing page 99\n",
      "Processing page 100\n",
      "Processing page 101\n",
      "Processing page 102\n",
      "Processing page 103\n",
      "Processing page 104\n",
      "Processing page 105\n",
      "Processing page 106\n",
      "Processing page 107\n",
      "Processing page 108\n",
      "Processing page 109\n",
      "Processing page 110\n",
      "Processing page 111\n",
      "Processing page 112\n",
      "Processing page 113\n",
      "Processing page 114\n",
      "Processing page 115\n",
      "Processing page 116\n",
      "Processing page 117\n",
      "Processing page 118\n",
      "Processing page 119\n",
      "Processing page 120\n",
      "Processing page 121\n",
      "Processing page 122\n",
      "Processing page 123\n",
      "Processing page 124\n",
      "Processing page 125\n",
      "Processing page 126\n",
      "Processing page 127\n",
      "Processing page 128\n",
      "Processing page 129\n",
      "Processing page 130\n",
      "Processing page 131\n",
      "Processing page 132\n",
      "Processing page 133\n",
      "Processing page 134\n",
      "Processing page 135\n",
      "Processing page 136\n",
      "Processing page 137\n",
      "Processing page 138\n",
      "Processing page 139\n",
      "Processing page 140\n",
      "Processing page 141\n",
      "Processing page 142\n",
      "Processing page 143\n",
      "Processing page 144\n",
      "Processing page 145\n",
      "Processing page 146\n",
      "Processing page 147\n",
      "Processing page 148\n",
      "Processing page 149\n",
      "Processing page 150\n",
      "Processing page 151\n",
      "Processing page 152\n",
      "Processing page 153\n",
      "Processing page 154\n",
      "Processing page 155\n",
      "Processing page 156\n",
      "Processing page 157\n",
      "Processing page 158\n",
      "Processing page 159\n",
      "Processing page 160\n",
      "Processing page 161\n",
      "Processing page 162\n",
      "Processing page 163\n",
      "Processing page 164\n",
      "Processing page 165\n",
      "Processing page 166\n",
      "Processing page 167\n",
      "Processing page 168\n",
      "Processing page 169\n",
      "Processing page 170\n",
      "Processing page 171\n",
      "Processing page 172\n",
      "Processing page 173\n",
      "Processing page 174\n",
      "Processing page 175\n",
      "Processing page 176\n",
      "Processing page 177\n",
      "Processing page 178\n",
      "Processing page 179\n",
      "Processing page 180\n",
      "Processing page 181\n",
      "Processing page 182\n",
      "Processing page 183\n",
      "Processing page 184\n",
      "Processing page 185\n",
      "Processing page 186\n",
      "Processing page 187\n",
      "Processing page 188\n",
      "Processing page 189\n",
      "Processing page 190\n",
      "Processing page 191\n",
      "Processing page 192\n",
      "Processing page 193\n",
      "Processing page 194\n",
      "Processing page 195\n",
      "Processing page 196\n",
      "Processing page 197\n",
      "Processing page 198\n",
      "Processing page 199\n",
      "Processing page 200\n",
      "Processing page 201\n",
      "Processing page 202\n",
      "Processing page 203\n",
      "Processing page 204\n",
      "Processing page 205\n",
      "Processing page 206\n",
      "Processing page 207\n",
      "Processing page 208\n",
      "Processing page 209\n",
      "Processing page 210\n",
      "Processing page 211\n",
      "Processing page 212\n",
      "Processing page 213\n",
      "Processing page 214\n",
      "Processing page 215\n",
      "Processing page 216\n",
      "Processing page 217\n",
      "Processing page 218\n",
      "Processing page 219\n",
      "Processing page 220\n",
      "Processing page 221\n",
      "Processing page 222\n",
      "Processing page 223\n",
      "Processing page 224\n",
      "Processing page 225\n",
      "Processing page 226\n",
      "Processing page 227\n",
      "Processing page 228\n",
      "Processing page 229\n",
      "Processing page 230\n",
      "Processing page 231\n",
      "Processing page 232\n",
      "Processing page 233\n",
      "Processing page 234\n",
      "Processing page 235\n",
      "Processing page 236\n",
      "Processing page 237\n",
      "Processing page 238\n",
      "Processing page 239\n",
      "Processing page 240\n",
      "Processing page 241\n",
      "Processing page 242\n",
      "Processing page 243\n",
      "Processing page 244\n",
      "Processing page 245\n",
      "Processing page 246\n",
      "Processing page 247\n",
      "Processing page 248\n",
      "Processing page 249\n",
      "Processing page 250\n",
      "Processing page 251\n",
      "Processing page 252\n",
      "Text extracted and saved to math_taw.txt\n",
      "Process took 1355.07 seconds\n"
     ]
    }
   ],
   "source": [
    "pdf_arabic_processor = PDFTextExtractor(dpi = 300, language='ara')\n",
    "\n",
    "pdf_arabic_processor.process_pdf('math_book_taw.pdf', 'math_taw.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAQERpc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import textwrap\n",
    "import time\n",
    "from llm import GeminiLLM\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "\n",
    "class ArabicTextCorrector:\n",
    "    \"\"\"\n",
    "    A class that provides functionality to correct Arabic text in chunks using an LLM model. \n",
    "    It handles reading, processing, splitting, and merging text files while allowing for corrections\n",
    "    with retry logic in case of errors during the correction process.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: GeminiLLM):\n",
    "        \"\"\"\n",
    "        Initializes the ArabicTextCorrector instance with a given model.\n",
    "\n",
    "        Args:\n",
    "            model (GeminiLLM): An instance of the GeminiLLM model used for text corrections.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "\n",
    "    def configure(self):\n",
    "        \"\"\"Configures the model for use. (Stub for potential future use)\"\"\"\n",
    "        pass\n",
    "\n",
    "    def read_text_from_file(self, file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Reads text from a file and returns it.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file to read from.\n",
    "\n",
    "        Returns:\n",
    "            str: The content of the file.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "\n",
    "    def save_text_to_file(self, text: str, file_path: str):\n",
    "        \"\"\"\n",
    "        Saves a string of text to a file, ensuring the directory exists.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to save.\n",
    "            file_path (str): The path to the file to save to.\n",
    "        \"\"\"\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(text)\n",
    "\n",
    "    def split_text(self, text: str, chunk_size: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Splits the text into chunks of a specified size.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be split.\n",
    "            chunk_size (int): The size of each chunk.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of text chunks.\n",
    "        \"\"\"\n",
    "        return textwrap.wrap(text, chunk_size)\n",
    "\n",
    "    def generate_correction_prompt(self, text: str, custom_prompt: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Generates a correction prompt for the model to process the text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be corrected.\n",
    "            custom_prompt (str): An optional custom prompt to be used for correction.\n",
    "\n",
    "        Returns:\n",
    "            str: A prompt that will be passed to the model for text correction.\n",
    "        \"\"\"\n",
    "        if custom_prompt:\n",
    "            return f\"{custom_prompt}\\n{text}\"\n",
    "        return f\"يرجى إعادة كتابة النص التالي بشكل صحيح دون أي ملاحظات أو توضيحات:\\n{text}\"\n",
    "\n",
    "    def correct_text(self, text: str, custom_prompt: str = None, max_retries: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        Corrects the provided text using the model, with retry logic in case of failure.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be corrected.\n",
    "            custom_prompt (str): An optional custom prompt for the correction.\n",
    "            max_retries (int): The maximum number of retry attempts in case of failure.\n",
    "\n",
    "        Returns:\n",
    "            str: The corrected text or an error message after the maximum retries.\n",
    "        \"\"\"\n",
    "        prompt = self.generate_correction_prompt(text, custom_prompt)\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model is not configured. Please call 'configure' first.\")\n",
    "        attempts = 0\n",
    "        while attempts < max_retries:\n",
    "            try:\n",
    "                response = self.model.generate_content(prompt)\n",
    "                return response\n",
    "            except Exception as e:\n",
    "                attempts += 1\n",
    "                print(f\"Error occurred while generating content (attempt {attempts}/{max_retries}): {e}\")\n",
    "                if attempts == max_retries:\n",
    "                    return f\"\\n\\n[تنبيه: لم يتم معالجة هذا الجزء بسبب خطأ بعد {max_retries} محاولات]\\n\\n{text}\"\n",
    "\n",
    "    def process_file(self, input_file: str, output_file: str, output_folder: str, chunk_size: int = 1000, custom_prompt: str = None):\n",
    "        \"\"\"\n",
    "        Processes the input file, splits it into chunks, corrects each chunk, and saves the output to a folder.\n",
    "\n",
    "        Args:\n",
    "            input_file (str): The path to the input text file.\n",
    "            output_file (str): The path to the final output file.\n",
    "            output_folder (str): The folder to save the corrected chunks.\n",
    "            chunk_size (int): The size of each chunk in characters.\n",
    "            custom_prompt (str): An optional custom prompt for text correction.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        text = self.read_text_from_file(input_file)\n",
    "        chunks = self.split_text(text, chunk_size)\n",
    "\n",
    "        corrected_text = \"\"\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            corrected_chunk = self.correct_text(chunk, custom_prompt)\n",
    "            corrected_text += corrected_chunk\n",
    "            output_path = os.path.join(output_folder, f'corrected_chunk_{i}.txt')\n",
    "            self.save_text_to_file(corrected_chunk, output_path)\n",
    "            print(f\"Processed chunk {i + 1} of {len(chunks)}\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        runtime = end_time - start_time\n",
    "        log_message = f\"{os.path.basename(__file__)} --> time taken: {runtime:.2f} seconds --> start of the run: {time.ctime(start_time)}\\n\"\n",
    "        self.log_runtime(log_message)\n",
    "\n",
    "    def save_chunks_without_processing(self, input_file: str, chunk_numbers: List[int], output_folder: str, chunk_size: int = 1000):\n",
    "        \"\"\"\n",
    "        Saves specified chunks of the input file without processing.\n",
    "\n",
    "        Args:\n",
    "            input_file (str): The path to the input text file.\n",
    "            chunk_numbers (List[int]): A list of chunk numbers to save.\n",
    "            output_folder (str): The folder to save the chunks.\n",
    "            chunk_size (int): The size of each chunk in characters.\n",
    "        \"\"\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        text = self.read_text_from_file(input_file)\n",
    "        chunks = self.split_text(text, chunk_size)\n",
    "\n",
    "        for index in chunk_numbers:\n",
    "            if index < 1 or index > len(chunks):\n",
    "                print(f\"Chunk number {index} is out of range.\")\n",
    "                continue\n",
    "            chunk = chunks[index - 1]\n",
    "            output_path = os.path.join(output_folder, f'corrected_chunk_{index}.txt')\n",
    "            self.save_text_to_file(chunk, output_path)\n",
    "            print(f\"Saved chunk {index} of {len(chunks)}\")\n",
    "\n",
    "    def log_runtime(self, log_message: str):\n",
    "        \"\"\"\n",
    "        Logs the runtime information into a file.\n",
    "\n",
    "        Args:\n",
    "            log_message (str): The message to be logged.\n",
    "        \"\"\"\n",
    "        with open(\"runtime.log\", 'a', encoding='utf-8') as log_file:\n",
    "            log_file.write(log_message)\n",
    "\n",
    "    def merge_chunks(self, input_folder: str, output_file: str):\n",
    "        \"\"\"\n",
    "        Merges all chunks in the input folder into a single file, sorted by chunk number.\n",
    "\n",
    "        Args:\n",
    "            input_folder (str): The folder containing chunk files.\n",
    "            output_file (str): The file to save the merged output.\n",
    "        \"\"\"\n",
    "        files = os.listdir(input_folder)\n",
    "        sorted_files = sorted(files, key=lambda x: int(re.search(r'corrected_chunk_(\\d+)\\.txt', x).group(1)))\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as output:\n",
    "            for file in sorted_files:\n",
    "                with open(os.path.join(input_folder, file), 'r', encoding='utf-8') as input_file:\n",
    "                    print(f\"Merging {file}\")\n",
    "                    output.write(input_file.read())\n",
    "                    output.write(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # Load environment variables\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "# Instantiate and configure GeminiLLM\n",
    "model = GeminiLLM(model_name='gemini-1.0-pro-latest')\n",
    "model.configure(api_key=gemini_api_key)\n",
    "corrector = ArabicTextCorrector(model)\n",
    "corrector.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk 1 of 167\n",
      "Processed chunk 2 of 167\n",
      "Processed chunk 3 of 167\n",
      "Processed chunk 4 of 167\n",
      "Processed chunk 5 of 167\n",
      "Processed chunk 6 of 167\n",
      "Processed chunk 7 of 167\n",
      "Processed chunk 8 of 167\n",
      "Processed chunk 9 of 167\n",
      "Processed chunk 10 of 167\n",
      "Processed chunk 11 of 167\n",
      "Processed chunk 12 of 167\n",
      "Processed chunk 13 of 167\n",
      "Processed chunk 14 of 167\n",
      "Processed chunk 15 of 167\n",
      "Processed chunk 16 of 167\n",
      "Processed chunk 17 of 167\n",
      "Processed chunk 18 of 167\n",
      "Processed chunk 19 of 167\n",
      "Processed chunk 20 of 167\n",
      "Processed chunk 21 of 167\n",
      "Processed chunk 22 of 167\n",
      "Processed chunk 23 of 167\n",
      "Processed chunk 24 of 167\n",
      "Processed chunk 25 of 167\n",
      "Processed chunk 26 of 167\n",
      "Processed chunk 27 of 167\n",
      "Processed chunk 28 of 167\n",
      "Processed chunk 29 of 167\n",
      "Processed chunk 30 of 167\n",
      "Processed chunk 31 of 167\n",
      "Processed chunk 32 of 167\n",
      "Processed chunk 33 of 167\n",
      "Error occurred while generating content (attempt 1/3): Unknown field for Candidate: finish_message\n",
      "Error occurred while generating content (attempt 2/3): Unknown field for Candidate: finish_message\n",
      "Processed chunk 34 of 167\n",
      "Processed chunk 35 of 167\n",
      "Processed chunk 36 of 167\n",
      "Processed chunk 37 of 167\n",
      "Processed chunk 38 of 167\n",
      "Processed chunk 39 of 167\n",
      "Processed chunk 40 of 167\n",
      "Processed chunk 41 of 167\n",
      "Processed chunk 42 of 167\n",
      "Processed chunk 43 of 167\n",
      "Processed chunk 44 of 167\n",
      "Processed chunk 45 of 167\n",
      "Processed chunk 46 of 167\n",
      "Processed chunk 47 of 167\n",
      "Processed chunk 48 of 167\n",
      "Processed chunk 49 of 167\n",
      "Processed chunk 50 of 167\n",
      "Processed chunk 51 of 167\n",
      "Processed chunk 52 of 167\n",
      "Processed chunk 53 of 167\n",
      "Processed chunk 54 of 167\n",
      "Processed chunk 55 of 167\n",
      "Processed chunk 56 of 167\n",
      "Processed chunk 57 of 167\n",
      "Processed chunk 58 of 167\n",
      "Processed chunk 59 of 167\n",
      "Processed chunk 60 of 167\n",
      "Processed chunk 61 of 167\n",
      "Processed chunk 62 of 167\n",
      "Processed chunk 63 of 167\n",
      "Processed chunk 64 of 167\n",
      "Processed chunk 65 of 167\n",
      "Processed chunk 66 of 167\n",
      "Processed chunk 67 of 167\n",
      "Processed chunk 68 of 167\n",
      "Processed chunk 69 of 167\n",
      "Processed chunk 70 of 167\n",
      "Processed chunk 71 of 167\n",
      "Processed chunk 72 of 167\n",
      "Processed chunk 73 of 167\n",
      "Processed chunk 74 of 167\n",
      "Processed chunk 75 of 167\n",
      "Processed chunk 76 of 167\n",
      "Processed chunk 77 of 167\n",
      "Processed chunk 78 of 167\n",
      "Processed chunk 79 of 167\n",
      "Processed chunk 80 of 167\n",
      "Processed chunk 81 of 167\n",
      "Processed chunk 82 of 167\n",
      "Processed chunk 83 of 167\n",
      "Processed chunk 84 of 167\n",
      "Processed chunk 85 of 167\n",
      "Processed chunk 86 of 167\n",
      "Processed chunk 87 of 167\n",
      "Processed chunk 88 of 167\n",
      "Processed chunk 89 of 167\n",
      "Processed chunk 90 of 167\n",
      "Processed chunk 91 of 167\n",
      "Processed chunk 92 of 167\n",
      "Processed chunk 93 of 167\n",
      "Processed chunk 94 of 167\n",
      "Processed chunk 95 of 167\n",
      "Processed chunk 96 of 167\n",
      "Processed chunk 97 of 167\n",
      "Processed chunk 98 of 167\n",
      "Processed chunk 99 of 167\n",
      "Processed chunk 100 of 167\n",
      "Processed chunk 101 of 167\n",
      "Processed chunk 102 of 167\n",
      "Processed chunk 103 of 167\n",
      "Processed chunk 104 of 167\n",
      "Processed chunk 105 of 167\n",
      "Processed chunk 106 of 167\n",
      "Processed chunk 107 of 167\n",
      "Processed chunk 108 of 167\n",
      "Processed chunk 109 of 167\n",
      "Processed chunk 110 of 167\n",
      "Processed chunk 111 of 167\n",
      "Error occurred while generating content (attempt 1/3): Unknown field for Candidate: finish_message\n",
      "Error occurred while generating content (attempt 2/3): Unknown field for Candidate: finish_message\n",
      "Processed chunk 112 of 167\n",
      "Processed chunk 113 of 167\n",
      "Error occurred while generating content (attempt 1/3): Unknown field for Candidate: finish_message\n",
      "Error occurred while generating content (attempt 2/3): Unknown field for Candidate: finish_message\n",
      "Error occurred while generating content (attempt 3/3): Unknown field for Candidate: finish_message\n",
      "Processed chunk 114 of 167\n",
      "Error occurred while generating content (attempt 1/3): Unknown field for Candidate: finish_message\n",
      "Processed chunk 115 of 167\n",
      "Processed chunk 116 of 167\n",
      "Processed chunk 117 of 167\n",
      "Processed chunk 118 of 167\n",
      "Processed chunk 119 of 167\n",
      "Processed chunk 120 of 167\n",
      "Processed chunk 121 of 167\n",
      "Processed chunk 122 of 167\n",
      "Processed chunk 123 of 167\n",
      "Processed chunk 124 of 167\n",
      "Processed chunk 125 of 167\n",
      "Processed chunk 126 of 167\n",
      "Processed chunk 127 of 167\n",
      "Processed chunk 128 of 167\n",
      "Processed chunk 129 of 167\n",
      "Processed chunk 130 of 167\n",
      "Processed chunk 131 of 167\n",
      "Processed chunk 132 of 167\n",
      "Processed chunk 133 of 167\n",
      "Error occurred while generating content (attempt 1/3): Unknown field for Candidate: finish_message\n",
      "Error occurred while generating content (attempt 2/3): Unknown field for Candidate: finish_message\n",
      "Error occurred while generating content (attempt 3/3): Unknown field for Candidate: finish_message\n",
      "Processed chunk 134 of 167\n",
      "Processed chunk 135 of 167\n",
      "Processed chunk 136 of 167\n",
      "Processed chunk 137 of 167\n",
      "Processed chunk 138 of 167\n",
      "Processed chunk 139 of 167\n",
      "Processed chunk 140 of 167\n",
      "Processed chunk 141 of 167\n",
      "Processed chunk 142 of 167\n",
      "Processed chunk 143 of 167\n",
      "Processed chunk 144 of 167\n",
      "Processed chunk 145 of 167\n",
      "Processed chunk 146 of 167\n",
      "Processed chunk 147 of 167\n",
      "Processed chunk 148 of 167\n",
      "Processed chunk 149 of 167\n",
      "Processed chunk 150 of 167\n",
      "Processed chunk 151 of 167\n",
      "Processed chunk 152 of 167\n",
      "Processed chunk 153 of 167\n",
      "Processed chunk 154 of 167\n",
      "Processed chunk 155 of 167\n",
      "Processed chunk 156 of 167\n",
      "Processed chunk 157 of 167\n",
      "Processed chunk 158 of 167\n",
      "Processed chunk 159 of 167\n",
      "Processed chunk 160 of 167\n",
      "Processed chunk 161 of 167\n",
      "Processed chunk 162 of 167\n",
      "Processed chunk 163 of 167\n",
      "Processed chunk 164 of 167\n",
      "Processed chunk 165 of 167\n",
      "Processed chunk 166 of 167\n",
      "Processed chunk 167 of 167\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Processing the file with custom prompt and logging runtime\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mcorrector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreligion_taw.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcorrected_religion_taw.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcorrect_religion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 134\u001b[0m, in \u001b[0;36mArabicTextCorrector.process_file\u001b[1;34m(self, input_file, output_file, output_folder, chunk_size, custom_prompt)\u001b[0m\n\u001b[0;32m    132\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    133\u001b[0m runtime \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m--> 134\u001b[0m log_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m --> time taken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mruntime\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds --> start of the run: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mctime(start_time)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_runtime(log_message)\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# Processing the file with custom prompt and logging runtime\n",
    "corrector.process_file(\n",
    "    input_file=\"math_taw.txt\", \n",
    "    output_file=\"corrected_math_taw.txt\", \n",
    "    output_folder=\"correct_math\",\n",
    "    chunk_size=1000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 113 of 167\n"
     ]
    }
   ],
   "source": [
    "# Saving specific chunks without processing\n",
    "\n",
    "chunk_numbers = [113]\n",
    "\n",
    "corrector.save_chunks_without_processing(input_file=\"religion_taw.txt\", chunk_numbers=chunk_numbers, output_folder=\"correct_religion\", chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging corrected_chunk_0.txt\n",
      "Merging corrected_chunk_1.txt\n",
      "Merging corrected_chunk_2.txt\n",
      "Merging corrected_chunk_3.txt\n",
      "Merging corrected_chunk_4.txt\n",
      "Merging corrected_chunk_5.txt\n",
      "Merging corrected_chunk_6.txt\n",
      "Merging corrected_chunk_7.txt\n",
      "Merging corrected_chunk_8.txt\n",
      "Merging corrected_chunk_9.txt\n",
      "Merging corrected_chunk_10.txt\n",
      "Merging corrected_chunk_11.txt\n",
      "Merging corrected_chunk_12.txt\n",
      "Merging corrected_chunk_13.txt\n",
      "Merging corrected_chunk_14.txt\n",
      "Merging corrected_chunk_15.txt\n",
      "Merging corrected_chunk_16.txt\n",
      "Merging corrected_chunk_17.txt\n",
      "Merging corrected_chunk_18.txt\n",
      "Merging corrected_chunk_19.txt\n",
      "Merging corrected_chunk_20.txt\n",
      "Merging corrected_chunk_21.txt\n",
      "Merging corrected_chunk_22.txt\n",
      "Merging corrected_chunk_23.txt\n",
      "Merging corrected_chunk_24.txt\n",
      "Merging corrected_chunk_25.txt\n",
      "Merging corrected_chunk_26.txt\n",
      "Merging corrected_chunk_27.txt\n",
      "Merging corrected_chunk_28.txt\n",
      "Merging corrected_chunk_29.txt\n",
      "Merging corrected_chunk_30.txt\n",
      "Merging corrected_chunk_31.txt\n",
      "Merging corrected_chunk_32.txt\n",
      "Merging corrected_chunk_33.txt\n",
      "Merging corrected_chunk_34.txt\n",
      "Merging corrected_chunk_35.txt\n",
      "Merging corrected_chunk_36.txt\n",
      "Merging corrected_chunk_37.txt\n",
      "Merging corrected_chunk_38.txt\n",
      "Merging corrected_chunk_39.txt\n",
      "Merging corrected_chunk_40.txt\n",
      "Merging corrected_chunk_41.txt\n",
      "Merging corrected_chunk_42.txt\n",
      "Merging corrected_chunk_43.txt\n",
      "Merging corrected_chunk_44.txt\n",
      "Merging corrected_chunk_45.txt\n",
      "Merging corrected_chunk_46.txt\n",
      "Merging corrected_chunk_47.txt\n",
      "Merging corrected_chunk_48.txt\n",
      "Merging corrected_chunk_49.txt\n",
      "Merging corrected_chunk_50.txt\n",
      "Merging corrected_chunk_51.txt\n",
      "Merging corrected_chunk_52.txt\n",
      "Merging corrected_chunk_53.txt\n",
      "Merging corrected_chunk_54.txt\n",
      "Merging corrected_chunk_55.txt\n",
      "Merging corrected_chunk_56.txt\n",
      "Merging corrected_chunk_57.txt\n",
      "Merging corrected_chunk_58.txt\n",
      "Merging corrected_chunk_59.txt\n",
      "Merging corrected_chunk_60.txt\n",
      "Merging corrected_chunk_61.txt\n",
      "Merging corrected_chunk_62.txt\n",
      "Merging corrected_chunk_63.txt\n",
      "Merging corrected_chunk_64.txt\n",
      "Merging corrected_chunk_65.txt\n",
      "Merging corrected_chunk_66.txt\n",
      "Merging corrected_chunk_67.txt\n",
      "Merging corrected_chunk_68.txt\n",
      "Merging corrected_chunk_69.txt\n",
      "Merging corrected_chunk_70.txt\n",
      "Merging corrected_chunk_71.txt\n",
      "Merging corrected_chunk_72.txt\n",
      "Merging corrected_chunk_73.txt\n",
      "Merging corrected_chunk_74.txt\n",
      "Merging corrected_chunk_75.txt\n",
      "Merging corrected_chunk_76.txt\n",
      "Merging corrected_chunk_77.txt\n",
      "Merging corrected_chunk_78.txt\n",
      "Merging corrected_chunk_79.txt\n",
      "Merging corrected_chunk_80.txt\n",
      "Merging corrected_chunk_81.txt\n",
      "Merging corrected_chunk_82.txt\n",
      "Merging corrected_chunk_83.txt\n",
      "Merging corrected_chunk_84.txt\n",
      "Merging corrected_chunk_85.txt\n",
      "Merging corrected_chunk_86.txt\n",
      "Merging corrected_chunk_87.txt\n",
      "Merging corrected_chunk_88.txt\n",
      "Merging corrected_chunk_89.txt\n",
      "Merging corrected_chunk_90.txt\n",
      "Merging corrected_chunk_91.txt\n",
      "Merging corrected_chunk_92.txt\n",
      "Merging corrected_chunk_93.txt\n",
      "Merging corrected_chunk_94.txt\n",
      "Merging corrected_chunk_95.txt\n",
      "Merging corrected_chunk_96.txt\n",
      "Merging corrected_chunk_97.txt\n",
      "Merging corrected_chunk_98.txt\n",
      "Merging corrected_chunk_99.txt\n",
      "Merging corrected_chunk_100.txt\n",
      "Merging corrected_chunk_101.txt\n",
      "Merging corrected_chunk_102.txt\n",
      "Merging corrected_chunk_103.txt\n",
      "Merging corrected_chunk_104.txt\n",
      "Merging corrected_chunk_105.txt\n",
      "Merging corrected_chunk_106.txt\n",
      "Merging corrected_chunk_107.txt\n",
      "Merging corrected_chunk_108.txt\n",
      "Merging corrected_chunk_109.txt\n",
      "Merging corrected_chunk_110.txt\n",
      "Merging corrected_chunk_111.txt\n",
      "Merging corrected_chunk_112.txt\n",
      "Merging corrected_chunk_113.txt\n",
      "Merging corrected_chunk_114.txt\n",
      "Merging corrected_chunk_115.txt\n",
      "Merging corrected_chunk_116.txt\n",
      "Merging corrected_chunk_117.txt\n",
      "Merging corrected_chunk_118.txt\n",
      "Merging corrected_chunk_119.txt\n",
      "Merging corrected_chunk_120.txt\n",
      "Merging corrected_chunk_121.txt\n",
      "Merging corrected_chunk_122.txt\n",
      "Merging corrected_chunk_123.txt\n",
      "Merging corrected_chunk_124.txt\n",
      "Merging corrected_chunk_125.txt\n",
      "Merging corrected_chunk_126.txt\n",
      "Merging corrected_chunk_127.txt\n",
      "Merging corrected_chunk_128.txt\n",
      "Merging corrected_chunk_129.txt\n",
      "Merging corrected_chunk_130.txt\n",
      "Merging corrected_chunk_131.txt\n",
      "Merging corrected_chunk_132.txt\n",
      "Merging corrected_chunk_133.txt\n",
      "Merging corrected_chunk_134.txt\n",
      "Merging corrected_chunk_135.txt\n",
      "Merging corrected_chunk_136.txt\n",
      "Merging corrected_chunk_137.txt\n",
      "Merging corrected_chunk_138.txt\n",
      "Merging corrected_chunk_139.txt\n",
      "Merging corrected_chunk_140.txt\n",
      "Merging corrected_chunk_141.txt\n",
      "Merging corrected_chunk_142.txt\n",
      "Merging corrected_chunk_143.txt\n",
      "Merging corrected_chunk_144.txt\n",
      "Merging corrected_chunk_145.txt\n",
      "Merging corrected_chunk_146.txt\n",
      "Merging corrected_chunk_147.txt\n",
      "Merging corrected_chunk_148.txt\n",
      "Merging corrected_chunk_149.txt\n",
      "Merging corrected_chunk_150.txt\n",
      "Merging corrected_chunk_151.txt\n",
      "Merging corrected_chunk_152.txt\n",
      "Merging corrected_chunk_153.txt\n",
      "Merging corrected_chunk_154.txt\n",
      "Merging corrected_chunk_155.txt\n",
      "Merging corrected_chunk_156.txt\n",
      "Merging corrected_chunk_157.txt\n",
      "Merging corrected_chunk_158.txt\n",
      "Merging corrected_chunk_159.txt\n",
      "Merging corrected_chunk_160.txt\n",
      "Merging corrected_chunk_161.txt\n",
      "Merging corrected_chunk_162.txt\n",
      "Merging corrected_chunk_163.txt\n",
      "Merging corrected_chunk_164.txt\n",
      "Merging corrected_chunk_165.txt\n",
      "Merging corrected_chunk_166.txt\n"
     ]
    }
   ],
   "source": [
    "#TODO: MERGE THE FILE AFTER MANUALLY EDITING UN-USABLE CHUNKS for both bio and hist\n",
    "\n",
    "##manual fix is done so now we can merge the files\n",
    "\n",
    "corrector.merge_chunks(input_folder=\"correct_religion\", output_file=\"corrected_religion.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##chroma class and its util classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chroma_text_processing import RecursiveCharacterTextSplitterAdapter, ChromaInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the interface\n",
    "text_splitter = RecursiveCharacterTextSplitterAdapter(chunk_size=200, chunk_overlap=20)\n",
    "\n",
    "chroma_interface = ChromaInterface(\"taw_religion\",\n",
    "                                   \"DB/chroma_db\",\n",
    "                                       text_splitter=text_splitter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(id=2d40b393-43fa-4837-a034-711c890415aa, name=taw_hist),\n",
       " Collection(id=86c8c4f8-7085-4c8e-a171-9c0d9b0cef11, name=taw_religion),\n",
       " Collection(id=ebf733d0-abe7-4952-8c42-9ee3538119d8, name=taw_bio)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_interface.client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the embeddings for the bio chunks\n",
    "\n",
    "chroma_interface.add_documents_from_files(\n",
    "    file_paths=[\"corrected_religion.txt\"],\n",
    "    metadatas=[{\"source\": \"corrected_religion.txt\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "['ومن مقتضيات معرفة دلالات النصوص الشرعية؛ العلم باللغة قواعدها، وصرفها، وبلاغتها، ومعرفة الألفاظ الخاصة والعامة.\\n\\nقال تعالى: «إنا أنزلناه قرآنا عربيا».', 'بدلاً من الأصناف المذكورة في الحديث؛ لأنها الأنفع للإخراج، والأيسر للمعطي، والأقرب لتحقيق مقصود النص.\\n\\nسادسًا: العلم باللغة العربية:\\n\\nفالعربية لغة القرآن الكريم، والسنة النبوية الشريفة.', 'في معاجمها: الستر مطلقًا؛ ويشمل المكان والثياب. ومن يثير مثل هذه الشبهة إنما يستغل بُعْدَ أبناء الإسلام عن لغتهم العربية.', '- حسين شمس الدين، السيرة النبوية (من البداية والنهاية لابن كثير)، تحقيق: مصطفى عبد الواحد، دار المعرفة للطباعة والنشر والتوزيع، بيروت.', 'والشباب. فائدة اللغة العربية لغة القرآن الكريم ووعاء الثقافة ورمز من رموز الهوية.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ask the user for input instead of hardcoding the query_text\n",
    "query_text = input(\"Please enter your query: \")\n",
    "\n",
    "# Execute the query using the user's input\n",
    "results = chroma_interface.query(query_text, n_results=5)\n",
    "\n",
    "# Print the results\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Result {i + 1}:\\n{result}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
